{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68022ab0-9484-443f-8b15-3ba3a55effbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45e2479-ba95-4b1a-a1be-4b6160e8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def pytorch_hpo_pipeline(\n",
    "    load_data,\n",
    "    run_hpo,\n",
    "    train_test,\n",
    "):\n",
    "    \"\"\"A `pipeline` to load data, load model, and train/evaluate the model.\"\"\"\n",
    "    train_dataloader, test_dataloader = load_data()\n",
    "    best_hparams = run_hpo(train_dataloader, test_dataloader)\n",
    "    train_test(best_hparams, train_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9b9f80-4966-4df4-8376-f5e1cbf1bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    \"\"\"A `step` to load the Fashion MNIST dataset as a tuple of torch Datasets.\"\"\"\n",
    "    batch_size = 64\n",
    "\n",
    "    # Download training data from open datasets.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Download test data from open datasets.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "@step\n",
    "def load_data() -> Output(\n",
    "    train_dataloader=DataLoader, test_dataloader=DataLoader\n",
    "):\n",
    "    train_dataloader, test_dataloader = get_mnist()\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f3d353-e4b0-4e45-8690-3e7afa86aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hpo_model(trial):\n",
    "    CLASSES = 10\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "    in_features = 28 * 28\n",
    "    layers.append(nn.Flatten())\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def objective(trial, train_dataloader, test_dataloader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Generate model\n",
    "    model = define_hpo_model(trial)\n",
    "    \n",
    "    # Get dataset\n",
    "    # train_dataloader, valid_dataloader = get_mnist()\n",
    "    \n",
    "    # Train model\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # lr=1e-3\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    test_accuracy = 100*correct\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "@step\n",
    "def run_hpo(\n",
    "    train_dataloader: DataLoader, \n",
    "    test_dataloader: DataLoader\n",
    ") -> dict:\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, train_dataloader, test_dataloader), n_trials=30, timeout=600)\n",
    "    \n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    best = trial.params\n",
    "    print(\"Found best hparam dict\")\n",
    "    print(best)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962e08bb-19a1-4333-8c7f-ae82c1da4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(hparam:dict):\n",
    "    CLASSES = 10\n",
    "    layers = []\n",
    "    in_features = 28 * 28\n",
    "    layers.append(nn.Flatten())\n",
    "    \n",
    "    print(hparam)\n",
    "    \n",
    "    for i in range(hparam['n_layers']):\n",
    "        out_features = hparam[f\"n_units_l{i}\"]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = hparam[f\"dropout_l{i}\"]\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4d56c1-b865-4871-8dcf-438bdbaab23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"A function to train a model for one epoch.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"A function to test a model on the validation / test dataset.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    test_accuracy = 100*correct\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089c46ef-775f-46a6-af7f-2f10ec9bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def train_test(\n",
    "    best_hparams: dict,\n",
    "    train_dataloader: DataLoader, \n",
    "    test_dataloader: DataLoader\n",
    ") -> Output(trained_model=nn.Module, test_acc=float):\n",
    "    \"\"\"A `step` to train and evaluate a torch model on given dataloaders.\"\"\"\n",
    "    \n",
    "    epochs = 5\n",
    "    \n",
    "    model = define_model(best_hparams)\n",
    "    \n",
    "    print(\"Training optimized model:\")\n",
    "    print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    lr = best_hparams['lr']\n",
    "    optimizer_name = best_hparams['optimizer_name']\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"Using optimized hparams\")\n",
    "    print(optimizer_name)\n",
    "    print(lr)\n",
    "    \n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    test_acc = 0\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_acc = test(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24973414-4318-4352-97bc-ac56fe181b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUnable to find ZenML repository in your current working directory (/home/dnth/Desktop/zenml_hpo) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run \u001b[0m\u001b[33mzenml init\u001b[33m.\u001b[0m\n",
      "\u001b[1;35mRunning without an active repository root.\u001b[0m\n",
      "\u001b[1;35mRunning unlisted pipeline on stack \u001b[0m\u001b[33mdefault\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_data\u001b[1;35m has finished in 0.113s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrun_hpo\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 13:59:39,073]\u001b[0m A new study created in memory with name: no-name-27cc2b47-740e-4bdf-814d-bd85b13a2eda\u001b[0m\n",
      "\u001b[32m[I 2022-11-28 13:59:45,261]\u001b[0m Trial 0 finished with value: 56.61000000000001 and parameters: {'n_layers': 3, 'n_units_l0': 43, 'dropout_l0': 0.2815373416933857, 'n_units_l1': 81, 'dropout_l1': 0.26580914247201665, 'n_units_l2': 9, 'dropout_l2': 0.3804910152892979, 'optimizer_name': 'SGD', 'lr': 0.018576818121802653}. Best is trial 0 with value: 56.61000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.153342 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 13:59:49,895]\u001b[0m Trial 1 finished with value: 68.19 and parameters: {'n_layers': 3, 'n_units_l0': 34, 'dropout_l0': 0.4666109869394437, 'n_units_l1': 75, 'dropout_l1': 0.2040654298545448, 'n_units_l2': 30, 'dropout_l2': 0.495855491006952, 'optimizer_name': 'RMSprop', 'lr': 0.0004397464868680228}. Best is trial 1 with value: 68.19.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.802256 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 13:59:54,039]\u001b[0m Trial 2 finished with value: 68.42 and parameters: {'n_layers': 1, 'n_units_l0': 79, 'dropout_l0': 0.4250115632505681, 'optimizer_name': 'SGD', 'lr': 0.0070359838295515256}. Best is trial 2 with value: 68.42.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.899483 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 13:59:58,233]\u001b[0m Trial 3 finished with value: 65.83 and parameters: {'n_layers': 1, 'n_units_l0': 111, 'dropout_l0': 0.2530293846407489, 'optimizer_name': 'RMSprop', 'lr': 0.02330745732885885}. Best is trial 2 with value: 68.42.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.936721 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:02,547]\u001b[0m Trial 4 finished with value: 11.77 and parameters: {'n_layers': 3, 'n_units_l0': 33, 'dropout_l0': 0.41725636527596455, 'n_units_l1': 116, 'dropout_l1': 0.3449336939187445, 'n_units_l2': 106, 'dropout_l2': 0.24341837513796566, 'optimizer_name': 'SGD', 'lr': 0.00019030212865495076}. Best is trial 2 with value: 68.42.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 2.302051 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:06,860]\u001b[0m Trial 5 finished with value: 74.33 and parameters: {'n_layers': 2, 'n_units_l0': 123, 'dropout_l0': 0.49366946147477736, 'n_units_l1': 84, 'dropout_l1': 0.38267825118764515, 'optimizer_name': 'RMSprop', 'lr': 0.006203909611540971}. Best is trial 5 with value: 74.33.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.669847 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:11,156]\u001b[0m Trial 6 finished with value: 79.4 and parameters: {'n_layers': 2, 'n_units_l0': 40, 'dropout_l0': 0.40012987884232554, 'n_units_l1': 68, 'dropout_l1': 0.42493149350613635, 'optimizer_name': 'RMSprop', 'lr': 0.0006669097261377514}. Best is trial 6 with value: 79.4.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.565038 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:15,475]\u001b[0m Trial 7 finished with value: 10.0 and parameters: {'n_layers': 3, 'n_units_l0': 83, 'dropout_l0': 0.2553191010289808, 'n_units_l1': 124, 'dropout_l1': 0.30430017022703204, 'n_units_l2': 24, 'dropout_l2': 0.34856634489874383, 'optimizer_name': 'SGD', 'lr': 0.00021497210663817205}. Best is trial 6 with value: 79.4.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302863 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:19,691]\u001b[0m Trial 8 finished with value: 9.879999999999999 and parameters: {'n_layers': 2, 'n_units_l0': 52, 'dropout_l0': 0.3537062026030516, 'n_units_l1': 22, 'dropout_l1': 0.4052377645630879, 'optimizer_name': 'SGD', 'lr': 3.813365084323978e-05}. Best is trial 6 with value: 79.4.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.300660 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:23,885]\u001b[0m Trial 9 finished with value: 81.24 and parameters: {'n_layers': 1, 'n_units_l0': 89, 'dropout_l0': 0.442552503307037, 'optimizer_name': 'RMSprop', 'lr': 0.003023017846521033}. Best is trial 9 with value: 81.24.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.509779 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:28,151]\u001b[0m Trial 10 finished with value: 31.95 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_l0': 0.3360917758294238, 'optimizer_name': 'Adam', 'lr': 1.1997305393254246e-05}. Best is trial 9 with value: 81.24.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.106076 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:32,477]\u001b[0m Trial 11 finished with value: 78.64999999999999 and parameters: {'n_layers': 2, 'n_units_l0': 94, 'dropout_l0': 0.3976209674432708, 'n_units_l1': 27, 'dropout_l1': 0.49083144897629744, 'optimizer_name': 'RMSprop', 'lr': 0.0015281495028733343}. Best is trial 9 with value: 81.24.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.551683 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:36,669]\u001b[0m Trial 12 finished with value: 83.02000000000001 and parameters: {'n_layers': 1, 'n_units_l0': 60, 'dropout_l0': 0.36412360421808027, 'optimizer_name': 'RMSprop', 'lr': 0.001646372802332384}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.471443 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:40,929]\u001b[0m Trial 13 finished with value: 20.74 and parameters: {'n_layers': 1, 'n_units_l0': 64, 'dropout_l0': 0.3400157088383581, 'optimizer_name': 'Adam', 'lr': 0.08009739238584779}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 20.7%, Avg loss: 1.998533 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:45,120]\u001b[0m Trial 14 finished with value: 82.23 and parameters: {'n_layers': 1, 'n_units_l0': 98, 'dropout_l0': 0.4559658793092641, 'optimizer_name': 'RMSprop', 'lr': 0.002092981200220419}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.475787 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:49,318]\u001b[0m Trial 15 finished with value: 82.91 and parameters: {'n_layers': 1, 'n_units_l0': 106, 'dropout_l0': 0.21464609434691512, 'optimizer_name': 'RMSprop', 'lr': 0.0012024531498919203}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.465186 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:53,517]\u001b[0m Trial 16 finished with value: 75.44999999999999 and parameters: {'n_layers': 1, 'n_units_l0': 68, 'dropout_l0': 0.222038739290704, 'optimizer_name': 'RMSprop', 'lr': 8.710024255639428e-05}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.710693 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:00:57,980]\u001b[0m Trial 17 finished with value: 81.34 and parameters: {'n_layers': 2, 'n_units_l0': 108, 'dropout_l0': 0.3043106114685663, 'n_units_l1': 39, 'dropout_l1': 0.4891847120361782, 'optimizer_name': 'Adam', 'lr': 0.0008442948328565954}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.506806 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:02,183]\u001b[0m Trial 18 finished with value: 81.24 and parameters: {'n_layers': 1, 'n_units_l0': 127, 'dropout_l0': 0.3780077124003605, 'optimizer_name': 'RMSprop', 'lr': 0.005792837477155618}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.517847 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:06,503]\u001b[0m Trial 19 finished with value: 74.11999999999999 and parameters: {'n_layers': 2, 'n_units_l0': 17, 'dropout_l0': 0.20582125918012248, 'n_units_l1': 9, 'dropout_l1': 0.2274015145762684, 'optimizer_name': 'RMSprop', 'lr': 0.00036363411035574954}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.836066 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:10,775]\u001b[0m Trial 20 finished with value: 74.96000000000001 and parameters: {'n_layers': 1, 'n_units_l0': 69, 'dropout_l0': 0.30683654802648536, 'optimizer_name': 'Adam', 'lr': 8.13958707295204e-05}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.769236 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:14,962]\u001b[0m Trial 21 finished with value: 82.56 and parameters: {'n_layers': 1, 'n_units_l0': 100, 'dropout_l0': 0.4630860528773349, 'optimizer_name': 'RMSprop', 'lr': 0.002184979503494007}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.471650 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:19,133]\u001b[0m Trial 22 finished with value: 82.55 and parameters: {'n_layers': 1, 'n_units_l0': 108, 'dropout_l0': 0.48643552947082147, 'optimizer_name': 'RMSprop', 'lr': 0.0030398341348543455}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.474891 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:23,327]\u001b[0m Trial 23 finished with value: 82.15 and parameters: {'n_layers': 1, 'n_units_l0': 56, 'dropout_l0': 0.3724276761798101, 'optimizer_name': 'RMSprop', 'lr': 0.0011315973636665008}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.490576 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:27,483]\u001b[0m Trial 24 finished with value: 75.9 and parameters: {'n_layers': 1, 'n_units_l0': 101, 'dropout_l0': 0.31401965367769613, 'optimizer_name': 'RMSprop', 'lr': 0.015367363502145131}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.687265 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:31,802]\u001b[0m Trial 25 finished with value: 81.46 and parameters: {'n_layers': 2, 'n_units_l0': 77, 'dropout_l0': 0.26865756875695407, 'n_units_l1': 48, 'dropout_l1': 0.3210221077924207, 'optimizer_name': 'RMSprop', 'lr': 0.003630516986127096}. Best is trial 12 with value: 83.02000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.490339 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:35,975]\u001b[0m Trial 26 finished with value: 83.31 and parameters: {'n_layers': 1, 'n_units_l0': 118, 'dropout_l0': 0.23036559984831573, 'optimizer_name': 'RMSprop', 'lr': 0.00137201635481431}. Best is trial 26 with value: 83.31.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.452150 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:40,163]\u001b[0m Trial 27 finished with value: 82.12 and parameters: {'n_layers': 1, 'n_units_l0': 119, 'dropout_l0': 0.23515505731670389, 'optimizer_name': 'RMSprop', 'lr': 0.0005706968679473509}. Best is trial 26 with value: 83.31.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.493544 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:44,490]\u001b[0m Trial 28 finished with value: 80.47999999999999 and parameters: {'n_layers': 2, 'n_units_l0': 113, 'dropout_l0': 0.2026168914676933, 'n_units_l1': 108, 'dropout_l1': 0.4389939683740725, 'optimizer_name': 'RMSprop', 'lr': 0.00022808188337547963}. Best is trial 26 with value: 83.31.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.543703 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 14:01:48,751]\u001b[0m Trial 29 finished with value: 79.55 and parameters: {'n_layers': 1, 'n_units_l0': 51, 'dropout_l0': 0.2876720747177252, 'optimizer_name': 'Adam', 'lr': 0.017565461501013218}. Best is trial 26 with value: 83.31.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.572266 \n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  30\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  30\n",
      "Best trial:\n",
      "  Value:  83.31\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_l0: 118\n",
      "    dropout_l0: 0.23036559984831573\n",
      "    optimizer_name: RMSprop\n",
      "    lr: 0.00137201635481431\n",
      "Found best hparam dict\n",
      "{'n_layers': 1, 'n_units_l0': 118, 'dropout_l0': 0.23036559984831573, 'optimizer_name': 'RMSprop', 'lr': 0.00137201635481431}\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrun_hpo\u001b[1;35m has finished in 2m9s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrain_test\u001b[1;35m has started.\u001b[0m\n",
      "{'n_layers': 1, 'n_units_l0': 118, 'dropout_l0': 0.23036559984831573, 'optimizer_name': 'RMSprop', 'lr': 0.00137201635481431}\n",
      "Training optimized model:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=118, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.23036559984831573, inplace=False)\n",
      "  (4): Linear(in_features=118, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n",
      "Using optimized hparams\n",
      "RMSprop\n",
      "0.00137201635481431\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296046  [    0/60000]\n",
      "loss: 0.804265  [ 6400/60000]\n",
      "loss: 0.460841  [12800/60000]\n",
      "loss: 0.683758  [19200/60000]\n",
      "loss: 0.498292  [25600/60000]\n",
      "loss: 0.462032  [32000/60000]\n",
      "loss: 0.484247  [38400/60000]\n",
      "loss: 0.596847  [44800/60000]\n",
      "loss: 0.659363  [51200/60000]\n",
      "loss: 0.505213  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.465367 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.288551  [    0/60000]\n",
      "loss: 0.433754  [ 6400/60000]\n",
      "loss: 0.356531  [12800/60000]\n",
      "loss: 0.479159  [19200/60000]\n",
      "loss: 0.405228  [25600/60000]\n",
      "loss: 0.421129  [32000/60000]\n",
      "loss: 0.407840  [38400/60000]\n",
      "loss: 0.469988  [44800/60000]\n",
      "loss: 0.544969  [51200/60000]\n",
      "loss: 0.491315  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.412842 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.283070  [    0/60000]\n",
      "loss: 0.374883  [ 6400/60000]\n",
      "loss: 0.226292  [12800/60000]\n",
      "loss: 0.398337  [19200/60000]\n",
      "loss: 0.345232  [25600/60000]\n",
      "loss: 0.349047  [32000/60000]\n",
      "loss: 0.383504  [38400/60000]\n",
      "loss: 0.498888  [44800/60000]\n",
      "loss: 0.482223  [51200/60000]\n",
      "loss: 0.424121  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.392343 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.290248  [    0/60000]\n",
      "loss: 0.337822  [ 6400/60000]\n",
      "loss: 0.266112  [12800/60000]\n",
      "loss: 0.352579  [19200/60000]\n",
      "loss: 0.341551  [25600/60000]\n",
      "loss: 0.398831  [32000/60000]\n",
      "loss: 0.388986  [38400/60000]\n",
      "loss: 0.462459  [44800/60000]\n",
      "loss: 0.450749  [51200/60000]\n",
      "loss: 0.377483  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.372164 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.226584  [    0/60000]\n",
      "loss: 0.388403  [ 6400/60000]\n",
      "loss: 0.261268  [12800/60000]\n",
      "loss: 0.327065  [19200/60000]\n",
      "loss: 0.315672  [25600/60000]\n",
      "loss: 0.385087  [32000/60000]\n",
      "loss: 0.299287  [38400/60000]\n",
      "loss: 0.407491  [44800/60000]\n",
      "loss: 0.387713  [51200/60000]\n",
      "loss: 0.450815  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.366825 \n",
      "\n",
      "Done!\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrain_test\u001b[1;35m has finished in 21.025s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mpytorch_hpo_pipeline-2022_11_28-13_59_38_762999\u001b[1;35m has finished in 2m31s.\u001b[0m\n",
      "\u001b[1;35mPipeline visualization can be seen in the ZenML Dashboard. Run \u001b[0m\u001b[33mzenml up\u001b[1;35m to see your pipeline!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pytorch_hpo_pipeline(\n",
    "    load_data=load_data(),\n",
    "    run_hpo=run_hpo(),\n",
    "    train_test=train_test(),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326d6d3-f432-48a8-aa0e-8127e2d9e746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf84e5-8767-47f3-969b-44fb89218241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0924c6-41b3-4200-b750-aaec9f02266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcb3d1-3acc-4e8e-ab46-c13770edae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
