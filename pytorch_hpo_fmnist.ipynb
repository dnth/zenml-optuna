{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68022ab0-9484-443f-8b15-3ba3a55effbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45e2479-ba95-4b1a-a1be-4b6160e8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def pytorch_hpo_pipeline(\n",
    "    load_data,\n",
    "    run_hpo,\n",
    "    train_test,\n",
    "):\n",
    "    \"\"\"A `pipeline` to load data, load model, and train/evaluate the model.\"\"\"\n",
    "    train_dataloader, test_dataloader = load_data()\n",
    "    best_hparams = run_hpo(train_dataloader, test_dataloader)\n",
    "    train_test(best_hparams, train_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9b9f80-4966-4df4-8376-f5e1cbf1bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    \n",
    "    batch_size = 64\n",
    "\n",
    "    # Download training data from open datasets.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Download test data from open datasets.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "@step\n",
    "def load_data() -> Output(\n",
    "    train_dataloader=DataLoader, test_dataloader=DataLoader\n",
    "):\n",
    "    \"\"\"A `step` to load the Fashion MNIST dataset as a tuple of torch Datasets.\"\"\"\n",
    "\n",
    "    train_dataloader, test_dataloader = get_mnist()\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f3d353-e4b0-4e45-8690-3e7afa86aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hpo_model(trial):\n",
    "    CLASSES = 10\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "    in_features = 28 * 28\n",
    "    layers.append(nn.Flatten())\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(f\"n_units_l{i}\", 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(f\"dropout_l{i}\", 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def objective(trial, train_dataloader, test_dataloader):\n",
    "\n",
    "    # Generate model\n",
    "    model = define_hpo_model(trial)\n",
    "    \n",
    "    # Train model\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    test_accuracy = 100*correct\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "@step\n",
    "def run_hpo(\n",
    "    train_dataloader: DataLoader, \n",
    "    test_dataloader: DataLoader\n",
    ") -> dict:\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, train_dataloader, test_dataloader), n_trials=30, timeout=600)\n",
    "    \n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    best = trial.params\n",
    "\n",
    "    print(\"Found best hyperparam dict from optimization work\")\n",
    "    print(best)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "962e08bb-19a1-4333-8c7f-ae82c1da4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(hparam:dict):\n",
    "    CLASSES = 10\n",
    "    layers = []\n",
    "    in_features = 28 * 28\n",
    "    layers.append(nn.Flatten())\n",
    "    \n",
    "    print(hparam)\n",
    "    \n",
    "    for i in range(hparam['n_layers']):\n",
    "        out_features = hparam[f\"n_units_l{i}\"]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = hparam[f\"dropout_l{i}\"]\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4d56c1-b865-4871-8dcf-438bdbaab23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"A function to train a model for one epoch.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"A function to test a model on the validation / test dataset.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    test_accuracy = 100*correct\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089c46ef-775f-46a6-af7f-2f10ec9bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def train_test(\n",
    "    best_hparams: dict,\n",
    "    train_dataloader: DataLoader, \n",
    "    test_dataloader: DataLoader\n",
    ") -> Output(trained_model=nn.Module, test_acc=float):\n",
    "    \"\"\"A `step` to train and evaluate a torch model on given dataloaders.\"\"\"\n",
    "    \n",
    "    epochs = 5\n",
    "    \n",
    "    model = define_model(best_hparams)\n",
    "    \n",
    "    print(\"Constructing optimized model:\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Using optimized hyperparams\")\n",
    "    print(best_hparams)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    lr = best_hparams['lr']\n",
    "    optimizer_name = best_hparams['optimizer_name']\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    test_acc = 0\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_acc = test(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return model, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24973414-4318-4352-97bc-ac56fe181b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUnable to find ZenML repository in your current working directory (/home/dnth/Desktop/zenml-optuna) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run \u001b[0m\u001b[33mzenml init\u001b[33m.\u001b[0m\n",
      "\u001b[1;35mRunning without an active repository root.\u001b[0m\n",
      "\u001b[1;35mRunning unlisted pipeline on stack \u001b[0m\u001b[33mdefault\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mload_data\u001b[1;35m has finished in 0.122s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrun_hpo\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:05,820]\u001b[0m A new study created in memory with name: no-name-c531756c-2561-4fa8-8a72-4b8b3737249a\u001b[0m\n",
      "\u001b[32m[I 2022-11-28 15:24:12,546]\u001b[0m Trial 0 finished with value: 71.55 and parameters: {'n_layers': 3, 'n_units_l0': 37, 'dropout_l0': 0.2825931202872253, 'n_units_l1': 97, 'dropout_l1': 0.3712839442432332, 'n_units_l2': 28, 'dropout_l2': 0.29779249195626956, 'optimizer_name': 'Adam', 'lr': 0.011292325923164617}. Best is trial 0 with value: 71.55.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.696968 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:17,413]\u001b[0m Trial 1 finished with value: 66.18 and parameters: {'n_layers': 2, 'n_units_l0': 64, 'dropout_l0': 0.3123391111812263, 'n_units_l1': 30, 'dropout_l1': 0.27063224887186577, 'optimizer_name': 'Adam', 'lr': 6.271506458338351e-05}. Best is trial 0 with value: 71.55.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.107685 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:21,991]\u001b[0m Trial 2 finished with value: 39.82 and parameters: {'n_layers': 1, 'n_units_l0': 10, 'dropout_l0': 0.3084514514979102, 'optimizer_name': 'Adam', 'lr': 2.2373143871057927e-05}. Best is trial 0 with value: 71.55.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.901742 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:26,571]\u001b[0m Trial 3 finished with value: 66.11 and parameters: {'n_layers': 1, 'n_units_l0': 7, 'dropout_l0': 0.4133913404517393, 'optimizer_name': 'Adam', 'lr': 0.00024656254426180476}. Best is trial 0 with value: 71.55.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.144856 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:31,257]\u001b[0m Trial 4 finished with value: 24.08 and parameters: {'n_layers': 2, 'n_units_l0': 63, 'dropout_l0': 0.4883719437760361, 'n_units_l1': 99, 'dropout_l1': 0.40618888050227675, 'optimizer_name': 'RMSprop', 'lr': 0.031087840732112897}. Best is trial 0 with value: 71.55.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 24.1%, Avg loss: 1.838406 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:36,192]\u001b[0m Trial 5 finished with value: 76.57000000000001 and parameters: {'n_layers': 3, 'n_units_l0': 83, 'dropout_l0': 0.45577728605124174, 'n_units_l1': 30, 'dropout_l1': 0.25761659393990644, 'n_units_l2': 126, 'dropout_l2': 0.2613242558434695, 'optimizer_name': 'Adam', 'lr': 0.0003869651045097696}. Best is trial 5 with value: 76.57000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.645832 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:41,168]\u001b[0m Trial 6 finished with value: 76.22 and parameters: {'n_layers': 3, 'n_units_l0': 66, 'dropout_l0': 0.41335701606245356, 'n_units_l1': 8, 'dropout_l1': 0.2917191636214642, 'n_units_l2': 110, 'dropout_l2': 0.3597125412802009, 'optimizer_name': 'Adam', 'lr': 0.0007244433294890441}. Best is trial 5 with value: 76.57000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.705339 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:45,964]\u001b[0m Trial 7 finished with value: 10.0 and parameters: {'n_layers': 3, 'n_units_l0': 118, 'dropout_l0': 0.21527593404486958, 'n_units_l1': 40, 'dropout_l1': 0.3118654844046039, 'n_units_l2': 8, 'dropout_l2': 0.217633737769976, 'optimizer_name': 'RMSprop', 'lr': 0.014069680737132356}. Best is trial 5 with value: 76.57000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.303998 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:50,715]\u001b[0m Trial 8 finished with value: 22.220000000000002 and parameters: {'n_layers': 2, 'n_units_l0': 113, 'dropout_l0': 0.24358106655768968, 'n_units_l1': 125, 'dropout_l1': 0.45534234053647965, 'optimizer_name': 'Adam', 'lr': 0.07201307316891992}. Best is trial 5 with value: 76.57000000000001.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 22.2%, Avg loss: 1.816598 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:55,131]\u001b[0m Trial 9 finished with value: 80.0 and parameters: {'n_layers': 1, 'n_units_l0': 25, 'dropout_l0': 0.373362714775197, 'optimizer_name': 'RMSprop', 'lr': 0.0024916870931530146}. Best is trial 9 with value: 80.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.574875 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:24:59,517]\u001b[0m Trial 10 finished with value: 62.83 and parameters: {'n_layers': 1, 'n_units_l0': 32, 'dropout_l0': 0.36351439413660186, 'optimizer_name': 'SGD', 'lr': 0.0037019020122377945}. Best is trial 9 with value: 80.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.280265 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:04,124]\u001b[0m Trial 11 finished with value: 81.28999999999999 and parameters: {'n_layers': 2, 'n_units_l0': 93, 'dropout_l0': 0.47807221311655024, 'n_units_l1': 59, 'dropout_l1': 0.2186370200862259, 'optimizer_name': 'RMSprop', 'lr': 0.0013311123023070385}. Best is trial 11 with value: 81.28999999999999.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.498327 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:08,538]\u001b[0m Trial 12 finished with value: 82.1 and parameters: {'n_layers': 1, 'n_units_l0': 94, 'dropout_l0': 0.3725861192386176, 'optimizer_name': 'RMSprop', 'lr': 0.002675739840814187}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.478580 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:13,211]\u001b[0m Trial 13 finished with value: 80.32000000000001 and parameters: {'n_layers': 2, 'n_units_l0': 94, 'dropout_l0': 0.48964234214615376, 'n_units_l1': 63, 'dropout_l1': 0.22974716790321875, 'optimizer_name': 'RMSprop', 'lr': 0.003286065853675785}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.563320 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:17,765]\u001b[0m Trial 14 finished with value: 77.16 and parameters: {'n_layers': 1, 'n_units_l0': 95, 'dropout_l0': 0.41681712142919736, 'optimizer_name': 'RMSprop', 'lr': 0.00011723810141532216}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.657954 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:22,372]\u001b[0m Trial 15 finished with value: 25.290000000000003 and parameters: {'n_layers': 2, 'n_units_l0': 126, 'dropout_l0': 0.44812292783867963, 'n_units_l1': 69, 'dropout_l1': 0.20379706100925404, 'optimizer_name': 'SGD', 'lr': 0.001161471769191213}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.228152 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:26,864]\u001b[0m Trial 16 finished with value: 78.97999999999999 and parameters: {'n_layers': 1, 'n_units_l0': 83, 'dropout_l0': 0.38247445489638265, 'optimizer_name': 'RMSprop', 'lr': 0.009620093455982918}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.552966 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:31,620]\u001b[0m Trial 17 finished with value: 81.66 and parameters: {'n_layers': 2, 'n_units_l0': 101, 'dropout_l0': 0.3282230828238434, 'n_units_l1': 67, 'dropout_l1': 0.4881758187593336, 'optimizer_name': 'RMSprop', 'lr': 0.0012766950097611936}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.491075 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:36,327]\u001b[0m Trial 18 finished with value: 79.24 and parameters: {'n_layers': 2, 'n_units_l0': 105, 'dropout_l0': 0.3271906563661023, 'n_units_l1': 86, 'dropout_l1': 0.47131035818725375, 'optimizer_name': 'RMSprop', 'lr': 0.000285227815017616}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.553215 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:40,744]\u001b[0m Trial 19 finished with value: 67.52 and parameters: {'n_layers': 1, 'n_units_l0': 82, 'dropout_l0': 0.2730028554497922, 'optimizer_name': 'SGD', 'lr': 0.006209654483371622}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.944555 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:45,415]\u001b[0m Trial 20 finished with value: 52.92 and parameters: {'n_layers': 2, 'n_units_l0': 55, 'dropout_l0': 0.34471791786888106, 'n_units_l1': 123, 'dropout_l1': 0.4204707131742388, 'optimizer_name': 'RMSprop', 'lr': 2.395729968862687e-05}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.503348 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:50,192]\u001b[0m Trial 21 finished with value: 80.61 and parameters: {'n_layers': 2, 'n_units_l0': 100, 'dropout_l0': 0.450352006164872, 'n_units_l1': 55, 'dropout_l1': 0.4961344337272596, 'optimizer_name': 'RMSprop', 'lr': 0.0012905887256005634}. Best is trial 12 with value: 82.1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.505599 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:54,947]\u001b[0m Trial 22 finished with value: 82.3 and parameters: {'n_layers': 2, 'n_units_l0': 78, 'dropout_l0': 0.27642562541175775, 'n_units_l1': 80, 'dropout_l1': 0.3527530065471359, 'optimizer_name': 'RMSprop', 'lr': 0.0007157728235360531}. Best is trial 22 with value: 82.3.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.486828 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:25:59,801]\u001b[0m Trial 23 finished with value: 78.7 and parameters: {'n_layers': 3, 'n_units_l0': 78, 'dropout_l0': 0.27703411066234684, 'n_units_l1': 81, 'dropout_l1': 0.328315876110598, 'n_units_l2': 68, 'dropout_l2': 0.4693516172667875, 'optimizer_name': 'RMSprop', 'lr': 0.0005378698980832049}. Best is trial 22 with value: 82.3.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.559357 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:04,523]\u001b[0m Trial 24 finished with value: 73.49 and parameters: {'n_layers': 2, 'n_units_l0': 110, 'dropout_l0': 0.3397166234084194, 'n_units_l1': 79, 'dropout_l1': 0.36021209389416725, 'optimizer_name': 'RMSprop', 'lr': 0.00011888340440784622}. Best is trial 22 with value: 82.3.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.698881 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:08,978]\u001b[0m Trial 25 finished with value: 83.48 and parameters: {'n_layers': 1, 'n_units_l0': 73, 'dropout_l0': 0.24536934603161373, 'optimizer_name': 'RMSprop', 'lr': 0.002076783936115092}. Best is trial 25 with value: 83.48.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.457751 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:13,425]\u001b[0m Trial 26 finished with value: 64.42 and parameters: {'n_layers': 1, 'n_units_l0': 55, 'dropout_l0': 0.20081050319114424, 'optimizer_name': 'SGD', 'lr': 0.0024721011144089385}. Best is trial 25 with value: 83.48.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.396542 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:17,903]\u001b[0m Trial 27 finished with value: 81.54 and parameters: {'n_layers': 1, 'n_units_l0': 71, 'dropout_l0': 0.24559901433155493, 'optimizer_name': 'RMSprop', 'lr': 0.004561475133838292}. Best is trial 25 with value: 83.48.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.494065 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:22,402]\u001b[0m Trial 28 finished with value: 75.33999999999999 and parameters: {'n_layers': 1, 'n_units_l0': 89, 'dropout_l0': 0.23703655843983137, 'optimizer_name': 'RMSprop', 'lr': 0.018553618102427272}. Best is trial 25 with value: 83.48.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.675442 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-28 15:26:26,857]\u001b[0m Trial 29 finished with value: 68.52000000000001 and parameters: {'n_layers': 1, 'n_units_l0': 47, 'dropout_l0': 0.2991821400337635, 'optimizer_name': 'RMSprop', 'lr': 0.034249674851051216}. Best is trial 25 with value: 83.48.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.891046 \n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  30\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  30\n",
      "Best trial:\n",
      "  Value:  83.48\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_l0: 73\n",
      "    dropout_l0: 0.24536934603161373\n",
      "    optimizer_name: RMSprop\n",
      "    lr: 0.002076783936115092\n",
      "Found best hyperparam dict from optimization work\n",
      "{'n_layers': 1, 'n_units_l0': 73, 'dropout_l0': 0.24536934603161373, 'optimizer_name': 'RMSprop', 'lr': 0.002076783936115092}\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrun_hpo\u001b[1;35m has finished in 2m21s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrain_test\u001b[1;35m has started.\u001b[0m\n",
      "{'n_layers': 1, 'n_units_l0': 73, 'dropout_l0': 0.24536934603161373, 'optimizer_name': 'RMSprop', 'lr': 0.002076783936115092}\n",
      "Constructing optimized model:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=73, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.24536934603161373, inplace=False)\n",
      "  (4): Linear(in_features=73, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n",
      "Using optimized hyperparams\n",
      "{'n_layers': 1, 'n_units_l0': 73, 'dropout_l0': 0.24536934603161373, 'optimizer_name': 'RMSprop', 'lr': 0.002076783936115092}\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310493  [    0/60000]\n",
      "loss: 0.737349  [ 6400/60000]\n",
      "loss: 0.495051  [12800/60000]\n",
      "loss: 0.651843  [19200/60000]\n",
      "loss: 0.548507  [25600/60000]\n",
      "loss: 0.505468  [32000/60000]\n",
      "loss: 0.483982  [38400/60000]\n",
      "loss: 0.665758  [44800/60000]\n",
      "loss: 0.595658  [51200/60000]\n",
      "loss: 0.564379  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.470250 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.356056  [    0/60000]\n",
      "loss: 0.487603  [ 6400/60000]\n",
      "loss: 0.385645  [12800/60000]\n",
      "loss: 0.465867  [19200/60000]\n",
      "loss: 0.445468  [25600/60000]\n",
      "loss: 0.485582  [32000/60000]\n",
      "loss: 0.348400  [38400/60000]\n",
      "loss: 0.522208  [44800/60000]\n",
      "loss: 0.575606  [51200/60000]\n",
      "loss: 0.490484  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.413649 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.295115  [    0/60000]\n",
      "loss: 0.370606  [ 6400/60000]\n",
      "loss: 0.332750  [12800/60000]\n",
      "loss: 0.407962  [19200/60000]\n",
      "loss: 0.391621  [25600/60000]\n",
      "loss: 0.418614  [32000/60000]\n",
      "loss: 0.340929  [38400/60000]\n",
      "loss: 0.592988  [44800/60000]\n",
      "loss: 0.396679  [51200/60000]\n",
      "loss: 0.514043  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.410763 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.324113  [    0/60000]\n",
      "loss: 0.325048  [ 6400/60000]\n",
      "loss: 0.313605  [12800/60000]\n",
      "loss: 0.395507  [19200/60000]\n",
      "loss: 0.380911  [25600/60000]\n",
      "loss: 0.389691  [32000/60000]\n",
      "loss: 0.350372  [38400/60000]\n",
      "loss: 0.545702  [44800/60000]\n",
      "loss: 0.445781  [51200/60000]\n",
      "loss: 0.451005  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.407241 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.339094  [    0/60000]\n",
      "loss: 0.339870  [ 6400/60000]\n",
      "loss: 0.283006  [12800/60000]\n",
      "loss: 0.405974  [19200/60000]\n",
      "loss: 0.282174  [25600/60000]\n",
      "loss: 0.424299  [32000/60000]\n",
      "loss: 0.374364  [38400/60000]\n",
      "loss: 0.416369  [44800/60000]\n",
      "loss: 0.362942  [51200/60000]\n",
      "loss: 0.446266  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.380948 \n",
      "\n",
      "Done!\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtrain_test\u001b[1;35m has finished in 22.688s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mpytorch_hpo_pipeline-2022_11_28-15_24_05_499394\u001b[1;35m has finished in 2m44s.\u001b[0m\n",
      "\u001b[1;35mPipeline visualization can be seen in the ZenML Dashboard. Run \u001b[0m\u001b[33mzenml up\u001b[1;35m to see your pipeline!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pytorch_hpo_pipeline(\n",
    "    load_data=load_data(),\n",
    "    run_hpo=run_hpo(),\n",
    "    train_test=train_test(),\n",
    ").run(unlisted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326d6d3-f432-48a8-aa0e-8127e2d9e746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf84e5-8767-47f3-969b-44fb89218241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0924c6-41b3-4200-b750-aaec9f02266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcb3d1-3acc-4e8e-ab46-c13770edae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('zenml-hpo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "28daa2db7ba61fb3b7fb1191c34e5dc56c2341f582f679c4c71e00b812b8bb12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
